{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrTW7nDwqASl",
    "tags": []
   },
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mR9Wgw8lqASo",
    "outputId": "3fd3e52c-1b27-4f85-d71e-354c0ee8b5c4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# import libraries\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "# nltk\n",
    "import nltk\n",
    "nl\n",
    "# The nltk packages should be better installed systemwide as described here:\n",
    "# https://www.nltk.org/data.html\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet') # download for lemmatization\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "# other models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u-KUTc1kqASq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterData.db')\n",
    "df = pd.read_sql_table('TextMessages', engine)\n",
    "X = df[[\"message\", \"original\", \"genre\"]]\n",
    "Y = df.drop(columns= [\"id\", \"message\", \"original\", \"genre\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in training and testing datasets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size = 0.05) # We drastically decrease the train_size to allow our GridSearch to run in a feasible amount of time\n",
    "\n",
    "# Code testing dataset - only 100 samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:10], Y[:10], train_size = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DBmXCwbqASr"
   },
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Xfgvt4eLqASr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Normalization\n",
    "    \n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation characters - this regex finds everything which is not a combination of letters\n",
    "    # and numbers and replaces it with a whitespace\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    \n",
    "    # Tokenization\n",
    "    \n",
    "    # Split into tokens\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # Part-of-speech tagging maybe useful here?\n",
    "    # Named Entity Recognition usefuk here?\n",
    "    \n",
    "    # Stemming - only keep the stem of a word, simple find and replace method which removes f.e. \"ing\"\n",
    "    # stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "    \n",
    "    # Lemmatization - more complex appraoch using dictionaries which can f.e. map \"is\" and \"was\" to \"be\"\n",
    "    # Lemmatize verbs by specifying pos\n",
    "    lemmed_verbs = [WordNetLemmatizer().lemmatize(w, pos='v') for w in words]\n",
    "    # Reduce nouns to their root form\n",
    "    lemmed_nouns = [WordNetLemmatizer().lemmatize(w) for w in lemmed_verbs]\n",
    "    return lemmed_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Yv1xIlnArACF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the average accuracy for each target column\n",
    "def calculate_accuracy(name, model, y_test, y_pred, _print=True):\n",
    "    columns = y_test.columns\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns = columns)\n",
    "    accuracy = (y_pred_df == y_test.reset_index().drop([\"index\"], axis = 1)).mean()\n",
    "    report = classification_report(y_true = y_test,\n",
    "                              y_pred = y_pred,\n",
    "                              target_names = list(y_test.columns),\n",
    "                            #  output_dict = True,\n",
    "                              zero_division = 0)\n",
    "    if _print:\n",
    "        print(f\"F1 score, recall and precision per category {name}: \\n\")\n",
    "        # print(f\"Average accuracy: {accuracy.mean()}\")\n",
    "        # print(accuracy)\n",
    "        print(report)\n",
    "    \n",
    "    return {'name' : name, 'model': model, 'report' : report}\n",
    "\n",
    "def print_results(results):\n",
    "    for result in results:\n",
    "        print(result[\"name\"])\n",
    "        print(result[\"model\"])\n",
    "        print(result[\"report\"])\n",
    "\n",
    "def print_parameters(classifier_name, classifier):\n",
    "    \"\"\"\n",
    "    This function prints all the available parameters, which a classifier consists off. These parameters can be used in GridSearch to optimize the classifier for our problem\n",
    "    \"\"\"\n",
    "    print(f\"Available parameters to optimize for {classifier_name}:\")\n",
    "    params = classifier.get_params().keys()\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define how many cores to use for training\n",
    "n_jobs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oYa2dVCeOljw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an empty array to store all the results and the models to find the best one in the end\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_6GvgH5qASs"
   },
   "source": [
    "# Native model without optimization (MultiOutputClassifier with RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KLuoHL9xqASs",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_random_forest(results, X_train, X_test,y_train, y_test):\n",
    "    # pipeline = Pipeline([\n",
    "    #         ('features', FeatureUnion([\n",
    "    #             ('text_pipeline', Pipeline([\n",
    "    #                 ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    #                 ('tfidf', TfidfTransformer())\n",
    "    #             ]))\n",
    "    #         ])),\n",
    "    #         ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    #     ])\n",
    "    random_forest_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(n_jobs = n_jobs)))\n",
    "    ])\n",
    "    random_forest_pipe.fit(X_train[\"message\"], y_train)\n",
    "    y_pred = random_forest_pipe.predict(X_test[\"message\"])\n",
    "    results.append(calculate_accuracy(\"MultiOutputClassifier RandomForest\", random_forest_pipe, y_test, y_pred,_print = False))\n",
    "    return results, random_forest_pipe\n",
    "\n",
    "results, random_forest_pipe = fit_random_forest(results, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1Skmc5Huqm0"
   },
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0VjADMcCqASx",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_knn(results, X_train, X_test,y_train, y_test):\n",
    "    knn_pipe = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', KNeighborsClassifier(n_jobs = n_jobs))\n",
    "        ])\n",
    "    knn_pipe.fit(X_train[\"message\"], y_train)\n",
    "    y_pred_knn = knn_pipe.predict(X_test[\"message\"])\n",
    "    results.append(calculate_accuracy(\"kNN\", knn_pipe, y_test, y_pred_knn,_print = False))\n",
    "    return results, knn_pipe\n",
    "\n",
    "results, knn_pipe = fit_knn(results, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5N0Od0AuuLn"
   },
   "source": [
    "# Decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cKw9pdNzuuvR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_decision_tree(results, X_train, X_test,y_train, y_test):\n",
    "    decision_tree_pipe = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "           ('tfidf', TfidfTransformer()),\n",
    "            ('clf', DecisionTreeClassifier())\n",
    "        ])\n",
    "    decision_tree_pipe.fit(X_train[\"message\"], y_train)\n",
    "    y_pred_decision_tree = decision_tree_pipe.predict(X_test[\"message\"])\n",
    "    results.append(calculate_accuracy(\"Decision Tree\", decision_tree_pipe, y_test, y_pred_decision_tree,_print = False))\n",
    "    return results, decision_tree_pipe\n",
    "\n",
    "results, decision_tree_pipe = fit_decision_tree(results, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiOutputClassifier RandomForest\n",
      "Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(tokenizer=<function tokenize at 0x7fa76ceae9e0>)),\n",
      "                ('tfidf', TfidfTransformer()),\n",
      "                ('clf',\n",
      "                 MultiOutputClassifier(estimator=RandomForestClassifier(n_jobs=8)))])\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.60      1.00      0.75         3\n",
      "               request       0.00      0.00      0.00         1\n",
      "                 offer       0.00      0.00      0.00         0\n",
      "           aid_related       0.20      1.00      0.33         1\n",
      "          medical_help       0.00      0.00      0.00         0\n",
      "      medical_products       0.00      0.00      0.00         0\n",
      "     search_and_rescue       0.00      0.00      0.00         0\n",
      "              security       0.00      0.00      0.00         0\n",
      "              military       0.00      0.00      0.00         0\n",
      "                 water       0.00      0.00      0.00         1\n",
      "                  food       0.00      0.00      0.00         0\n",
      "               shelter       0.00      0.00      0.00         1\n",
      "              clothing       0.00      0.00      0.00         0\n",
      "                 money       0.00      0.00      0.00         0\n",
      "        missing_people       0.00      0.00      0.00         0\n",
      "              refugees       0.00      0.00      0.00         0\n",
      "                 death       0.00      0.00      0.00         0\n",
      "             other_aid       0.00      0.00      0.00         0\n",
      "infrastructure_related       0.00      0.00      0.00         0\n",
      "             transport       0.00      0.00      0.00         0\n",
      "             buildings       0.00      0.00      0.00         0\n",
      "           electricity       0.00      0.00      0.00         0\n",
      "                 tools       0.00      0.00      0.00         0\n",
      "             hospitals       0.00      0.00      0.00         0\n",
      "                 shops       0.00      0.00      0.00         0\n",
      "           aid_centers       0.00      0.00      0.00         0\n",
      "  other_infrastructure       0.00      0.00      0.00         0\n",
      "       weather_related       0.20      1.00      0.33         1\n",
      "                floods       0.00      0.00      0.00         0\n",
      "                 storm       0.00      0.00      0.00         1\n",
      "                  fire       0.00      0.00      0.00         0\n",
      "            earthquake       0.00      0.00      0.00         0\n",
      "                  cold       0.00      0.00      0.00         0\n",
      "         other_weather       0.00      0.00      0.00         0\n",
      "         direct_report       0.00      0.00      0.00         1\n",
      "\n",
      "             micro avg       0.25      0.50      0.33        10\n",
      "             macro avg       0.03      0.09      0.04        10\n",
      "          weighted avg       0.22      0.50      0.29        10\n",
      "           samples avg       0.25      0.40      0.27        10\n",
      "\n",
      "kNN\n",
      "Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(tokenizer=<function tokenize at 0x7fa76ceae9e0>)),\n",
      "                ('tfidf', TfidfTransformer()),\n",
      "                ('clf', KNeighborsClassifier(n_jobs=8))])\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.60      1.00      0.75         3\n",
      "               request       0.00      0.00      0.00         1\n",
      "                 offer       0.00      0.00      0.00         0\n",
      "           aid_related       0.20      1.00      0.33         1\n",
      "          medical_help       0.00      0.00      0.00         0\n",
      "      medical_products       0.00      0.00      0.00         0\n",
      "     search_and_rescue       0.00      0.00      0.00         0\n",
      "              security       0.00      0.00      0.00         0\n",
      "              military       0.00      0.00      0.00         0\n",
      "                 water       0.00      0.00      0.00         1\n",
      "                  food       0.00      0.00      0.00         0\n",
      "               shelter       0.00      0.00      0.00         1\n",
      "              clothing       0.00      0.00      0.00         0\n",
      "                 money       0.00      0.00      0.00         0\n",
      "        missing_people       0.00      0.00      0.00         0\n",
      "              refugees       0.00      0.00      0.00         0\n",
      "                 death       0.00      0.00      0.00         0\n",
      "             other_aid       0.00      0.00      0.00         0\n",
      "infrastructure_related       0.00      0.00      0.00         0\n",
      "             transport       0.00      0.00      0.00         0\n",
      "             buildings       0.00      0.00      0.00         0\n",
      "           electricity       0.00      0.00      0.00         0\n",
      "                 tools       0.00      0.00      0.00         0\n",
      "             hospitals       0.00      0.00      0.00         0\n",
      "                 shops       0.00      0.00      0.00         0\n",
      "           aid_centers       0.00      0.00      0.00         0\n",
      "  other_infrastructure       0.00      0.00      0.00         0\n",
      "       weather_related       0.00      0.00      0.00         1\n",
      "                floods       0.00      0.00      0.00         0\n",
      "                 storm       0.00      0.00      0.00         1\n",
      "                  fire       0.00      0.00      0.00         0\n",
      "            earthquake       0.00      0.00      0.00         0\n",
      "                  cold       0.00      0.00      0.00         0\n",
      "         other_weather       0.00      0.00      0.00         0\n",
      "         direct_report       0.00      0.00      0.00         1\n",
      "\n",
      "             micro avg       0.40      0.40      0.40        10\n",
      "             macro avg       0.02      0.06      0.03        10\n",
      "          weighted avg       0.20      0.40      0.26        10\n",
      "           samples avg       0.40      0.33      0.31        10\n",
      "\n",
      "Decision Tree\n",
      "Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(tokenizer=<function tokenize at 0x7fa76ceae9e0>)),\n",
      "                ('tfidf', TfidfTransformer()),\n",
      "                ('clf', DecisionTreeClassifier())])\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.60      1.00      0.75         3\n",
      "               request       0.00      0.00      0.00         1\n",
      "                 offer       0.00      0.00      0.00         0\n",
      "           aid_related       0.00      0.00      0.00         1\n",
      "          medical_help       0.00      0.00      0.00         0\n",
      "      medical_products       0.00      0.00      0.00         0\n",
      "     search_and_rescue       0.00      0.00      0.00         0\n",
      "              security       0.00      0.00      0.00         0\n",
      "              military       0.00      0.00      0.00         0\n",
      "                 water       0.00      0.00      0.00         1\n",
      "                  food       0.00      0.00      0.00         0\n",
      "               shelter       0.00      0.00      0.00         1\n",
      "              clothing       0.00      0.00      0.00         0\n",
      "                 money       0.00      0.00      0.00         0\n",
      "        missing_people       0.00      0.00      0.00         0\n",
      "              refugees       0.00      0.00      0.00         0\n",
      "                 death       0.00      0.00      0.00         0\n",
      "             other_aid       0.00      0.00      0.00         0\n",
      "infrastructure_related       0.00      0.00      0.00         0\n",
      "             transport       0.00      0.00      0.00         0\n",
      "             buildings       0.00      0.00      0.00         0\n",
      "           electricity       0.00      0.00      0.00         0\n",
      "                 tools       0.00      0.00      0.00         0\n",
      "             hospitals       0.00      0.00      0.00         0\n",
      "                 shops       0.00      0.00      0.00         0\n",
      "           aid_centers       0.00      0.00      0.00         0\n",
      "  other_infrastructure       0.00      0.00      0.00         0\n",
      "       weather_related       0.00      0.00      0.00         1\n",
      "                floods       0.00      0.00      0.00         0\n",
      "                 storm       0.00      0.00      0.00         1\n",
      "                  fire       0.00      0.00      0.00         0\n",
      "            earthquake       0.00      0.00      0.00         0\n",
      "                  cold       0.00      0.00      0.00         0\n",
      "         other_weather       0.00      0.00      0.00         0\n",
      "         direct_report       0.00      0.00      0.00         1\n",
      "\n",
      "             micro avg       0.60      0.30      0.40        10\n",
      "             macro avg       0.02      0.03      0.02        10\n",
      "          weighted avg       0.18      0.30      0.22        10\n",
      "           samples avg       0.60      0.30      0.36        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9ZbSLokO-ye",
    "tags": []
   },
   "source": [
    "# Improve models using GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0z9wmeTCSMKb"
   },
   "source": [
    "## MultiOutputClassifier + RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 15.0,\n",
       " 16.0,\n",
       " 17.0,\n",
       " 18.0,\n",
       " 19.0,\n",
       " 20.0,\n",
       " 21.0,\n",
       " 22.0,\n",
       " 23.0,\n",
       " 24.0,\n",
       " 25.0,\n",
       " 26.0,\n",
       " 27.0,\n",
       " 28.0,\n",
       " 29.0,\n",
       " 30.0,\n",
       " 31.0,\n",
       " 32.0,\n",
       " 33.0,\n",
       " 34.0,\n",
       " 35.0,\n",
       " 36.0,\n",
       " 37.0,\n",
       " 38.0,\n",
       " 39.0,\n",
       " 40.0,\n",
       " 41.0,\n",
       " 42.0,\n",
       " 43.0,\n",
       " 44.0,\n",
       " 45.0,\n",
       " 46.0,\n",
       " 47.0,\n",
       " 48.0,\n",
       " 49.0,\n",
       " 50.0,\n",
       " 51.0,\n",
       " 52.0,\n",
       " 53.0,\n",
       " 54.0,\n",
       " 55.0,\n",
       " 56.0,\n",
       " 57.0,\n",
       " 58.0,\n",
       " 59.0,\n",
       " 60.0,\n",
       " 61.0,\n",
       " 62.0,\n",
       " 63.0,\n",
       " 64.0,\n",
       " 65.0,\n",
       " 66.0,\n",
       " 67.0,\n",
       " 68.0,\n",
       " 69.0,\n",
       " 70.0,\n",
       " 71.0,\n",
       " 72.0,\n",
       " 73.0,\n",
       " 74.0,\n",
       " 75.0,\n",
       " 76.0,\n",
       " 77.0,\n",
       " 78.0,\n",
       " 79.0,\n",
       " 80.0,\n",
       " 81.0,\n",
       " 82.0,\n",
       " 83.0,\n",
       " 84.0,\n",
       " 85.0,\n",
       " 86.0,\n",
       " 87.0,\n",
       " 88.0,\n",
       " 89.0,\n",
       " 90.0,\n",
       " 91.0,\n",
       " 92.0,\n",
       " 93.0,\n",
       " 94.0,\n",
       " 95.0,\n",
       " 96.0,\n",
       " 97.0,\n",
       " 98.0,\n",
       " 99.0,\n",
       " 100.0,\n",
       " 101.0,\n",
       " 102.0,\n",
       " 103.0,\n",
       " 104.0,\n",
       " 105.0,\n",
       " 106.0,\n",
       " 107.0,\n",
       " 108.0,\n",
       " 109.0,\n",
       " 110.0,\n",
       " 111.0,\n",
       " 112.0,\n",
       " 113.0,\n",
       " 114.0,\n",
       " 115.0,\n",
       " 116.0,\n",
       " 117.0,\n",
       " 118.0,\n",
       " 119.0,\n",
       " 120.0,\n",
       " 121.0,\n",
       " 122.0,\n",
       " 123.0,\n",
       " 124.0,\n",
       " 125.0,\n",
       " 126.0,\n",
       " 127.0,\n",
       " 128.0,\n",
       " 129.0,\n",
       " 130.0,\n",
       " 131.0,\n",
       " 132.0,\n",
       " 133.0,\n",
       " 134.0,\n",
       " 135.0,\n",
       " 136.0,\n",
       " 137.0,\n",
       " 138.0,\n",
       " 139.0,\n",
       " 140.0,\n",
       " 141.0,\n",
       " 142.0,\n",
       " 143.0,\n",
       " 144.0,\n",
       " 145.0,\n",
       " 146.0,\n",
       " 147.0,\n",
       " 148.0,\n",
       " 149.0,\n",
       " 150.0,\n",
       " 151.0,\n",
       " 152.0,\n",
       " 153.0,\n",
       " 154.0,\n",
       " 155.0,\n",
       " 156.0,\n",
       " 157.0,\n",
       " 158.0,\n",
       " 159.0,\n",
       " 160.0,\n",
       " 161.0,\n",
       " 162.0,\n",
       " 163.0,\n",
       " 164.0,\n",
       " 165.0,\n",
       " 166.0,\n",
       " 167.0,\n",
       " 168.0,\n",
       " 169.0,\n",
       " 170.0,\n",
       " 171.0,\n",
       " 172.0,\n",
       " 173.0,\n",
       " 174.0,\n",
       " 175.0,\n",
       " 176.0,\n",
       " 177.0,\n",
       " 178.0,\n",
       " 179.0,\n",
       " 180.0,\n",
       " 181.0,\n",
       " 182.0,\n",
       " 183.0,\n",
       " 184.0,\n",
       " 185.0,\n",
       " 186.0,\n",
       " 187.0,\n",
       " 188.0,\n",
       " 189.0,\n",
       " 190.0,\n",
       " 191.0,\n",
       " 192.0,\n",
       " 193.0,\n",
       " 194.0,\n",
       " 195.0,\n",
       " 196.0,\n",
       " 197.0,\n",
       " 198.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parallel(n_jobs)(delayed(sqrt)(i ** 2) for i in range(199))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using dask - maybe overkill here\n",
    "# https://joblib.readthedocs.io/en/latest/auto_examples/parallel/distributed_backend_simple.html\n",
    "from dask.distributed import Client\n",
    "# Set up a Dask client with 4 threads and 1 worker\n",
    "client = Client(processes=False, threads_per_worker=8, n_workers=1)\n",
    "\n",
    "# # Run grid search using joblib and a Dask backend\n",
    "# with joblib.parallel_backend(\"____\"):\n",
    "#     engrid.fit(x_train, y_train)\n",
    "\n",
    "# plot_enet(*enet_path(x_test, y_test, eps=5e-5, fit_intercept=False,\n",
    "#                     l1_ratio=engrid.best_params_[\"l1_ratio\"])[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "gEtxLFsfPDif",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/martin/.local/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 391, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"/usr/lib64/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\nAttributeError: 'WordListCorpusReader' object has no attribute '_unload'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(calculate_accuracy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiOutputClassifier Random Forest CV\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv_rf, y_test, y_pred_mo_rf_cv,_print \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results, cv_rf\n\u001b[0;32m---> 35\u001b[0m results, cv_rf \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_forest_gridsearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_forest_pipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [27], line 27\u001b[0m, in \u001b[0;36mrandom_forest_gridsearch\u001b[0;34m(results, random_forest_pipe, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m      6\u001b[0m parameters_mo_rf \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# vect\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf__estimator__max_depth\u001b[39m\u001b[38;5;124m'\u001b[39m : [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     24\u001b[0m cv_rf \u001b[38;5;241m=\u001b[39m GridSearchCV(random_forest_pipe, param_grid\u001b[38;5;241m=\u001b[39mparameters_mo_rf, \n\u001b[1;32m     25\u001b[0m                      \u001b[38;5;66;03m# Automatically stops GridSearch and raises occuring errors\u001b[39;00m\n\u001b[1;32m     26\u001b[0m                      error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_jobs \u001b[38;5;241m=\u001b[39m n_jobs)\n\u001b[0;32m---> 27\u001b[0m tqdm(\u001b[43mcv_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# with joblib.parallel_backend(\"dask\"):\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     tqdm(cv_rf.fit(X_train[\"message\"], y_train))\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Parallel(n_jobs)(cv_rf.fit(X_train[\"message\"], y_train))\u001b[39;00m\n\u001b[1;32m     31\u001b[0m y_pred_mo_rf_cv \u001b[38;5;241m=\u001b[39m cv_rf\u001b[38;5;241m.\u001b[39mpredict(X_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib64/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-571526' coro=<DaskDistributedBackend._collect() running at /home/martin/.local/lib/python3.10/site-packages/joblib/_dask.py:202> wait_for=<Future pending cb=[Task.task_wakeup()]> cb=[IOLoop.add_future.<locals>.<lambda>() at /home/martin/.local/lib/python3.10/site-packages/tornado/ioloop.py:688]>\n"
     ]
    }
   ],
   "source": [
    "#print_parameters(\"Random Forest\", random_forest_pipe)\n",
    "from tqdm import tqdm\n",
    "\n",
    "def random_forest_gridsearch(results, random_forest_pipe, X_train, X_test, y_train, y_test):\n",
    "    # Define a dictionary with all the parameters\n",
    "    parameters_mo_rf = {\n",
    "        # vect\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "\n",
    "        # tfidf\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "        'tfidf__norm' : ['l1', 'l2'],\n",
    "        #  'tfidf__use_idf' : [True, False],\n",
    "        # 'tfidf__smooth_idf': [True, False],\n",
    "        # 'tfidf__sublinear_tf' : [True, False],\n",
    "\n",
    "        # clf\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "        'clf__estimator__criterion' : ['gini', 'entropy'],\n",
    "        'clf__estimator__n_estimators': [50, 100, 150, 200],\n",
    "        'clf__estimator__max_depth' : [None, 5, 10],\n",
    "    }\n",
    "    cv_rf = GridSearchCV(random_forest_pipe, param_grid=parameters_mo_rf, \n",
    "                         # Automatically stops GridSearch and raises occuring errors\n",
    "                         error_score=\"raise\", n_jobs = n_jobs)\n",
    "    print(\"Pipeline created - GridSearch starts now\")\n",
    "    tqdm(cv_rf.fit(X_train[\"message\"], y_train))\n",
    "    print(\"GridSearch finished\")\n",
    "    # with joblib.parallel_backend(\"dask\"):\n",
    "    #     tqdm(cv_rf.fit(X_train[\"message\"], y_train))\n",
    "    # Parallel(n_jobs)(cv_rf.fit(X_train[\"message\"], y_train))\n",
    "    y_pred_mo_rf_cv = cv_rf.predict(X_test[\"message\"])\n",
    "    results.append(calculate_accuracy(\"MultiOutputClassifier Random Forest CV\", cv_rf, y_test, y_pred_mo_rf_cv,_print = False))\n",
    "    return results, cv_rf\n",
    "\n",
    "results, cv_rf = random_forest_gridsearch(results, random_forest_pipe, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC2PzZIgSTda"
   },
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMrFqFS0XuJZ",
    "outputId": "eda65e65-896d-403b-aa40-4cbff666d9b0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_parameters(\"kNN\", knn_pipe)\n",
    "\n",
    "def kNN_gridsearch(results, knn_pipe, X_train, X_test, y_train, y_test):\n",
    "    parameters_knn = {\n",
    "        # vect\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "\n",
    "        # tfidf\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "        'tfidf__norm' : ['l1', 'l2'],\n",
    "      #  'tfidf__use_idf' : [True, False],\n",
    "      #  'tfidf__smooth_idf': [True, False],\n",
    "      #  'tfidf__sublinear_tf' : [True, False],\n",
    "\n",
    "        # clf\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "        'clf__n_neighbors' : [3, 5, 8],\n",
    "        'clf__weights' : ['uniform', 'distance'],\n",
    "        'clf__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "\n",
    "    }\n",
    "    cv_knn = GridSearchCV(knn_pipe, param_grid=parameters_knn, n_jobs=n_jobs, \n",
    "                          # Automatically stops GridSearch and raises occuring errors\n",
    "                          error_score=\"raise\") \n",
    "    cv_knn.fit(X_train[\"message\"], y_train)\n",
    "    y_pred_knn_cv = cv_knn.predict(X_test[\"message\"])\n",
    "    results.append(calculate_accuracy(\"kNN CV\", cv_knn, y_test, y_pred_knn_cv,_print = False))\n",
    "    return results, cv_knn\n",
    "\n",
    "results, cv_knn = kNN_gridsearch(results, knn_pipe, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_true = y_test,\n",
    "                              y_pred = y_pred,\n",
    "                              target_names = list(y_test.columns),\n",
    "                              output_dict = True,\n",
    "                              zero_division = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk4UwgQrSX6E"
   },
   "source": [
    "# Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9uUyuoWJcZz",
    "outputId": "f2de889a-2fe6-4c62-f76f-b83ed3a28d43",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIg_hFQZSaMJ"
   },
   "source": [
    "As we can see, the models performed all very similar. Only the decision tree model is a bit worse compared to the other ones. Surprisingly, our unoptimized orginal model with a MultiOutpuClassfier and a RandomForestClassifier performed best. Therefore we can assume that the standard model configuration fits good to our problem and the optimization attempt only leads us away from the optimum. 94% is a quite good result so we can stick with that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pu1IyfRDJcRh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = results[0]['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FX0t5mtRO8-"
   },
   "source": [
    "Now that we found the best model configuration, we retrain the model with 80% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R87lA-AcQejZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X, Y, train_size = 0.80)\n",
    "best_model.fit(X_train_new[\"message\"], y_train_new)\n",
    "y_pred_final = best_model.predict(X_test_new[\"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrzCQwmdqASx"
   },
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQCqCcQCqASy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_params = best_model.get_params()\n",
    "model = best_model\n",
    "\n",
    "fileObj = open('model_params.obj', 'wb')\n",
    "pickle.dump(model_params,fileObj)\n",
    "fileObj.close()\n",
    "\n",
    "fileObj = open('model.obj', 'wb')\n",
    "pickle.dump(model,fileObj)\n",
    "fileObj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RFRSYRjqASy"
   },
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAlLehW1qASy",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ML Pipeline Preparation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
