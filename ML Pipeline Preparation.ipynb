{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrTW7nDwqASl"
   },
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mR9Wgw8lqASo",
    "outputId": "3fd3e52c-1b27-4f85-d71e-354c0ee8b5c4"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# import libraries\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "# nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') # download for lemmatization\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "# other models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u-KUTc1kqASq"
   },
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterData.db')\n",
    "df = pd.read_sql_table('TextMessages', engine)\n",
    "X = df[[\"message\", \"original\", \"genre\"]]\n",
    "Y = df.drop(columns= [\"id\", \"message\", \"original\", \"genre\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DBmXCwbqASr"
   },
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Xfgvt4eLqASr"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Normalization\n",
    "    \n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation characters - this regex finds everything which is not a combination of letters\n",
    "    # and numbers and replaces it with a whitespace\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    \n",
    "    # Tokenization\n",
    "    \n",
    "    # Split into tokens\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # Part-of-speech tagging maybe useful here?\n",
    "    # Named Entity Recognition usefuk here?\n",
    "    \n",
    "    # Stemming - only keep the stem of a word, simple find and replace method which removes f.e. \"ing\"\n",
    "    # stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "    \n",
    "    # Lemmatization - more complex appraoch using dictionaries which can f.e. map \"is\" and \"was\" to \"be\"\n",
    "    # Lemmatize verbs by specifying pos\n",
    "    lemmed_verbs = [WordNetLemmatizer().lemmatize(w, pos='v') for w in words]\n",
    "    # Reduce nouns to their root form\n",
    "    lemmed_nouns = [WordNetLemmatizer().lemmatize(w) for w in lemmed_verbs]\n",
    "    return lemmed_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5LpueGxsqyc_"
   },
   "outputs": [],
   "source": [
    "# Split the data in training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size = 0.05) # We drastically decrease the train_size to allow our GridSearch to run in a feasible amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Yv1xIlnArACF"
   },
   "outputs": [],
   "source": [
    "# Calculate the average accuracy for each target column\n",
    "def print_acc(name, model, y_test, y_pred):\n",
    "    columns = y_test.columns\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns = columns)\n",
    "    accuracy = (y_pred_df == y_test.reset_index().drop([\"index\"], axis = 1)).mean()\n",
    "    report = classification_report(y_true = y_test,\n",
    "                              y_pred = y_pred,\n",
    "                              target_names = list(y_test.columns),\n",
    "                            #  output_dict = True,\n",
    "                              zero_division = 0)\n",
    "    print(f\"F1 score, recall and precision per category {name}: \\n\")\n",
    "    # print(f\"Average accuracy: {accuracy.mean()}\")\n",
    "    # print(accuracy)\n",
    "    print(report)\n",
    "    \n",
    "    return {'name' : name, 'model': model, 'report' : report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oYa2dVCeOljw"
   },
   "outputs": [],
   "source": [
    "# Create an empty array to store all the results and the models to find the best one in the end\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_6GvgH5qASs"
   },
   "source": [
    "# Native model without optimization (MultiOutputClassifier with RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KLuoHL9xqASs"
   },
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#         ('features', FeatureUnion([\n",
    "\n",
    "#             ('text_pipeline', Pipeline([\n",
    "#                 ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "#                 ('tfidf', TfidfTransformer())\n",
    "#             ]))\n",
    "#         ])),\n",
    "\n",
    "#         ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "#     ])\n",
    "\n",
    "def fit_random_forest(results, X_train[], y_train):\n",
    "    random_forest_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    random_forest_pipe.fit(X_train[\"message\"], y_train)\n",
    "    y_pred = random_forest_pipe.predict(X_test[\"message\"])\n",
    "    results.append(print_acc(\"MultiOutputClassifier RandomForest\", random_forest_pipe, y_test, y_pred))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea7ozGZ0qASv",
    "outputId": "6582686e-dcb8-4e11-8e39-d81425843385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score, recall and precision per category MultiOutputClassifier RandomForest: \n",
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.80      0.96      0.87     19083\n",
      "               request       0.82      0.40      0.54      4236\n",
      "                 offer       0.00      0.00      0.00       112\n",
      "           aid_related       0.73      0.62      0.67     10306\n",
      "          medical_help       0.69      0.02      0.04      1981\n",
      "      medical_products       0.00      0.00      0.00      1258\n",
      "     search_and_rescue       0.00      0.00      0.00       684\n",
      "              security       0.00      0.00      0.00       445\n",
      "              military       0.50      0.00      0.00       811\n",
      "                 water       0.97      0.05      0.09      1599\n",
      "                  food       0.84      0.41      0.55      2781\n",
      "               shelter       0.84      0.16      0.27      2197\n",
      "              clothing       0.00      0.00      0.00       390\n",
      "                 money       0.00      0.00      0.00       570\n",
      "        missing_people       0.00      0.00      0.00       282\n",
      "              refugees       0.50      0.00      0.00       825\n",
      "                 death       1.00      0.00      0.00      1132\n",
      "             other_aid       0.61      0.00      0.01      3283\n",
      "infrastructure_related       0.50      0.00      0.00      1616\n",
      "             transport       0.92      0.01      0.02      1147\n",
      "             buildings       0.80      0.01      0.02      1269\n",
      "           electricity       0.00      0.00      0.00       504\n",
      "                 tools       0.00      0.00      0.00       156\n",
      "             hospitals       0.00      0.00      0.00       265\n",
      "                 shops       0.00      0.00      0.00       113\n",
      "           aid_centers       0.00      0.00      0.00       286\n",
      "  other_infrastructure       0.00      0.00      0.00      1105\n",
      "       weather_related       0.87      0.54      0.67      6925\n",
      "                floods       0.92      0.16      0.27      2045\n",
      "                 storm       0.81      0.10      0.17      2320\n",
      "                  fire       0.00      0.00      0.00       272\n",
      "            earthquake       0.89      0.61      0.73      2310\n",
      "                  cold       0.00      0.00      0.00       506\n",
      "         other_weather       0.71      0.00      0.01      1313\n",
      "         direct_report       0.73      0.27      0.40      4807\n",
      "\n",
      "             micro avg       0.80      0.44      0.57     78934\n",
      "             macro avg       0.44      0.12      0.15     78934\n",
      "          weighted avg       0.72      0.44      0.47     78934\n",
      "           samples avg       0.69      0.44      0.49     78934\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1Skmc5Huqm0"
   },
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0VjADMcCqASx"
   },
   "outputs": [],
   "source": [
    "# knn_pipe = Pipeline([\n",
    "#         ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "#         ('tfidf', TfidfTransformer()),\n",
    "#         ('clf', KNeighborsClassifier())\n",
    "#     ])\n",
    "# knn_pipe.fit(X_train[\"message\"], y_train)\n",
    "# y_pred_knn = knn_pipe.predict(X_test[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbUvK8ZPu66T",
    "outputId": "526d36bb-92b5-484d-855f-d1529c22d394"
   },
   "outputs": [],
   "source": [
    "# results.append(print_acc(\"kNN\", knn_pipe, y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5N0Od0AuuLn"
   },
   "source": [
    "# Decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cKw9pdNzuuvR"
   },
   "outputs": [],
   "source": [
    "# decision_tree_pipe = Pipeline([\n",
    "#         ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "#         ('tfidf', TfidfTransformer()),\n",
    "#         ('clf', DecisionTreeClassifier())\n",
    "#     ])\n",
    "# decision_tree_pipe.fit(X_train[\"message\"], y_train)\n",
    "# y_pred_decision_tree = decision_tree_pipe.predict(X_test[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8o7h0swu-b8",
    "outputId": "3118dcdf-07d7-4fc0-b3ce-72ca55fde454"
   },
   "outputs": [],
   "source": [
    "# results.append(print_acc(\"Decision Tree\", decision_tree_pipe, y_test, y_pred_decision_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyxC-47wIKoO"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Jt18UDVNISCi"
   },
   "outputs": [],
   "source": [
    "# random_forest_only_pipe = Pipeline([\n",
    "#         ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "#         ('tfidf', TfidfTransformer()),\n",
    "#         ('clf', RandomForestClassifier())\n",
    "#     ])\n",
    "# random_forest_only_pipe.fit(X_train[\"message\"], y_train)\n",
    "# y_pred_random_forest_only = random_forest_only_pipe.predict(X_test[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lILTiPA6OSmX",
    "outputId": "916967b6-e8d6-4c52-b98d-281ef1617c08"
   },
   "outputs": [],
   "source": [
    "# results.append(print_acc(\"Random Forest\", random_forest_only_pipe, y_test, y_pred_random_forest_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOIIBf0eR8NU",
    "outputId": "2016bfc2-c161-480a-bbcc-9c87a1191d6c"
   },
   "outputs": [],
   "source": [
    "# for result in results:\n",
    "#   print(result[\"name\"])\n",
    "#   print(result[\"accuracy\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9ZbSLokO-ye"
   },
   "source": [
    "# Improve models using GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0z9wmeTCSMKb"
   },
   "source": [
    "## MultiOutputClassifier + RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8RwevSxO-WI",
    "outputId": "0f505ef6-2a9d-4d5a-d1d5-84d0485ec9a4"
   },
   "outputs": [],
   "source": [
    "# Check for available parameters to optimize\n",
    "# random_forest_pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gEtxLFsfPDif"
   },
   "outputs": [],
   "source": [
    "# parameters_mo_rf = {\n",
    "#     # vect\n",
    "#     # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "    \n",
    "#     # tfidf\n",
    "#     # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "#     'tfidf__norm' : ['l1', 'l2'],\n",
    "#   #  'tfidf__use_idf' : [True, False],\n",
    "#    # 'tfidf__smooth_idf': [True, False],\n",
    "#    # 'tfidf__sublinear_tf' : [True, False],\n",
    "\n",
    "#     # clf\n",
    "#     # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "#     'clf__estimator__criterion' : ['gini', 'entropy'],\n",
    "#     'clf__estimator__n_estimators': [50, 100, 150, 200],\n",
    "#     'clf__estimator__max_depth' : [None, 5, 10],\n",
    "# }\n",
    "\n",
    "# cv_parameters_mo_rf = GridSearchCV(random_forest_pipe, param_grid=parameters_mo_rf) \n",
    "# cv_parameters_mo_rf.fit(X_train[\"message\"], y_train)\n",
    "# y_pred_mo_rf_cv = cv_parameters_mo_rf.predict(X_test[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIE9fH5iXrX8",
    "outputId": "c65a85c2-7bc3-45dc-d2af-3745988dfa8e"
   },
   "outputs": [],
   "source": [
    "# results.append(print_acc(\"MultiOutputClassifier Random Forest CV\", cv_parameters_mo_rf, y_test, y_pred_mo_rf_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC2PzZIgSTda"
   },
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unz31Y-1XuO2",
    "outputId": "04074bfc-aba6-47a5-bf11-0db13229c336"
   },
   "outputs": [],
   "source": [
    "# knn_pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMrFqFS0XuJZ",
    "outputId": "eda65e65-896d-403b-aa40-4cbff666d9b0"
   },
   "outputs": [],
   "source": [
    "# parameters_knn = {\n",
    "#     # vect\n",
    "#     # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "    \n",
    "#     # tfidf\n",
    "#     # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "#     'tfidf__norm' : ['l1', 'l2'],\n",
    "#   #  'tfidf__use_idf' : [True, False],\n",
    "#   #  'tfidf__smooth_idf': [True, False],\n",
    "#   #  'tfidf__sublinear_tf' : [True, False],\n",
    "\n",
    "#     # clf\n",
    "#     # https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "#     'clf__n_neighbors' : [3, 5, 8],\n",
    "#     'clf__weights' : ['uniform', 'distance'],\n",
    "#     'clf__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "\n",
    "# }\n",
    "\n",
    "# cv_knn = GridSearchCV(knn_pipe, param_grid=parameters_knn) \n",
    "# cv_knn.fit(X_train[\"message\"], y_train)\n",
    "# y_pred_knn_cv = cv_knn.predict(X_test[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWNJIWb6Z7Gg",
    "outputId": "faed75e0-a8dc-469a-db55-5be41a934d4f"
   },
   "outputs": [],
   "source": [
    "# results.append(print_acc(\"kNN CV\", cv_knn, y_test, y_pred_mo_rf_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_true = y_test,\n",
    "                              y_pred = y_pred,\n",
    "                              target_names = list(y_test.columns),\n",
    "                              output_dict = True,\n",
    "                              zero_division = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk4UwgQrSX6E"
   },
   "source": [
    "# Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9uUyuoWJcZz",
    "outputId": "f2de889a-2fe6-4c62-f76f-b83ed3a28d43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiOutputClassifier RandomForest\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean())\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "  print(result[\"name\"])\n",
    "  print(result[\"accuracy\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIg_hFQZSaMJ"
   },
   "source": [
    "As we can see, the models performed all very similar. Only the decision tree model is a bit worse compared to the other ones. Surprisingly, our unoptimized orginal model with a MultiOutpuClassfier and a RandomForestClassifier performed best. Therefore we can assume that the standard model configuration fits good to our problem and the optimization attempt only leads us away from the optimum. 94% is a quite good result so we can stick with that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pu1IyfRDJcRh"
   },
   "outputs": [],
   "source": [
    "best_model = results[0]['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FX0t5mtRO8-"
   },
   "source": [
    "Now that we found the best model configuration, we retrain the model with 80% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R87lA-AcQejZ"
   },
   "outputs": [],
   "source": [
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X, Y, train_size = 0.80)\n",
    "best_model.fit(X_train_new[\"message\"], y_train_new)\n",
    "y_pred_final = best_model.predict(X_test_new[\"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrzCQwmdqASx"
   },
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQCqCcQCqASy"
   },
   "outputs": [],
   "source": [
    "model_params = best_model.get_params()\n",
    "model = best_model\n",
    "\n",
    "fileObj = open('model_params.obj', 'wb')\n",
    "pickle.dump(model_params,fileObj)\n",
    "fileObj.close()\n",
    "\n",
    "fileObj = open('model.obj', 'wb')\n",
    "pickle.dump(model,fileObj)\n",
    "fileObj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RFRSYRjqASy"
   },
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAlLehW1qASy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ML Pipeline Preparation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
